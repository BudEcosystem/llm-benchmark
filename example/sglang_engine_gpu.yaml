args:
  model-path: 
    - meta-llama/Meta-Llama-3.1-8B-Instruct
    # - microsoft/Phi-3.5-mini-instruct
    # - Qwen/Qwen2-0.5B-Instruct
    # - Qwen/Qwen2-7B-Instruct
  port: 8988
  # tensor-parallel-size:
  #   - 1
    # - 2
  # max-running-requests: #auto calculated
  #   - 100
  #   - 200
  # num-continuous-decode-steps:
  #   - 1
  #   - 2
  #   - 4
  # attention-backend :
  #   - flashinfer
  #   - triton
  # sampling-backend:
  #   - pytorch
  #   - flashinfer
  # chunked-prefill-size:
  #   - 4096
  #   - 8192
  #   - 16384
  # max-prefill-tokens:
  #   - 16384
  #   - 8192
  # schedule-policy:
  #   - lpm
  #   - random
  #   - fcfs
  #   - dfs-weight
  # schedule-conservativeness:
  #   - 0.5
  #   - 1.0
  #   - 2.0
  # stream-interval:
  #   - 1
  #   - 2
  # enable-torch-compile:
  #   - true
  #   - false

envs:
  CUDA_VISIBLE_DEVICES: "0,3"

run_config:
  mean_input_tokens:
    - 50
    # - 200
    # - 550
    # - 1000
    - 8192
    - 16384
    - 32768
    # - 65536
  mean_output_tokens:
    # - 128
    - 250
    # - 550
    # - 1000
    # - 1500
  num_concurrent_requests: 
    # - 1
    - 10
    # - 50
    # - 100
    # - 200
    # - 500
